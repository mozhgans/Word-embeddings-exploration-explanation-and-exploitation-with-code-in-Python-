{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "onehotvectorizing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwftFxMUPE_r"
      },
      "source": [
        "One-hot encoding (CountVectorizing):\n",
        "The most basic and naive method for transforming words into vectors is to count occurrence of each word in each document, isnâ€™t it? Such an approach is called countvectorizing or one-hot encoding (dependent on the literature).\n",
        "The idea is to collect a set of documents (they can be words, sentences, paragraphs or even articles) and count the occurrence of every word in them. Strictly speaking, the columns of the resulting matrix are words and the rows are documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-E1SXImPDef",
        "outputId": "e2a73dea-ee9a-4d15-8915-672ff30c1455"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = [\n",
        "          'Text of first document.',\n",
        "          'Text of the second document made longer.',\n",
        "          'Number three.',\n",
        "          'This is number four.',\n",
        "]\n",
        "# learn the vocabulary and store CountVectorizer sparse matrix in X\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(X.toarray())\n",
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer2.get_feature_names())\n",
        "\n",
        "print(X2.toarray())\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['document', 'first', 'four', 'is', 'longer', 'made', 'number', 'of', 'second', 'text', 'the', 'this', 'three']\n",
            "[[1 1 0 0 0 0 0 1 0 1 0 0 0]\n",
            " [1 0 0 0 1 1 0 1 1 1 1 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
            " [0 0 1 1 0 0 1 0 0 0 0 1 0]]\n",
            "['document made', 'first document', 'is number', 'made longer', 'number four', 'number three', 'of first', 'of the', 'second document', 'text of', 'the second', 'this is']\n",
            "[[0 1 0 0 0 0 1 0 0 1 0 0]\n",
            " [1 0 0 1 0 0 0 1 1 1 1 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 1 0 1 0 0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu6uVF1qAGCV",
        "outputId": "63669d5f-25d7-461c-a6b9-26117c230418"
      },
      "source": [
        "corpus"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Text of first document.',\n",
              " 'Text of the second document made longer.',\n",
              " 'Number three.',\n",
              " 'This is number four.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TaOpB42AM9C",
        "outputId": "608d1c71-52a9-4307-baa4-5e6ec5f725fe"
      },
      "source": [
        "X"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4x13 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 17 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ54SKuJBQyS"
      },
      "source": [
        "other way of same code as bove:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORwbqLF2BUia",
        "outputId": "77c44c54-7cec-4c5d-fa87-1f7a0cb42b6c"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = [\n",
        "          'Text of first document.',\n",
        "          'Text of the second document made longer.',\n",
        "          'Number three.',\n",
        "          'This is number four.',\n",
        "]\n",
        "# learn the vocabulary and store CountVectorizer sparse matrix in X\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "# columns of X correspond to the result of this method\n",
        "vectorizer.get_feature_names() == (\n",
        "    ['document', 'first', 'four', 'is', 'longer',\n",
        "     'made', 'number', 'of', 'second', 'text',\n",
        "     'the', 'this', 'three'])\n",
        "# retrieving the matrix in the numpy form\n",
        "X.toarray()\n",
        "# transforming a new document according to learn vocabulary\n",
        "vectorizer.transform(['A new document.']).toarray()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wILDswAoBb0B"
      },
      "source": [
        "source:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
        "\n",
        "https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMEBpQqYBwzV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}